Title: A Bit About Null Hypothesis Significance Testing (NHST)
Slug: a-bit-of-NHST
Date: '2022-02-27'
Category: Theory
Tags: 'NHST', 'Sampling Distributions', 'p-values'
Author: PB
Summary: A Brief Review of NHST

## Motivation
Recently at work, this topic has come up related to some experimentation. During the course of the discussions, it became pretty clear that the concepts of [Null Hypothesis Significance Testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing) (NHST) create a great deal of confusion.  Concepts such as [p-values](https://en.wikipedia.org/wiki/P-value), [Confidence Intervals](https://en.wikipedia.org/wiki/Confidence_interval), [Type I & Type II Errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) amongst others are misintrepreted, even by experts in the field. To that end, I am quite certain I'll probably mess up something as well and look forward to the correction. :) 

But the discussions today motivated me to put a few notes down about NHST to at least address a few of the core concepts, specifically the notion of a "[Sampling Distribution](https://en.wikipedia.org/wiki/Sampling_distribution)" and **p-values**. I'm hoping to address some more advanced aspects including some of the objections to this, Let's get to the math by discussing the problem this is intended to solve.

## The Problem
At a high-level, the major issue that needs to be solved is that you have data generated by some random process and you want to test your belief about some aspect of this process. For the sake of the example, let's assume that the data, $\mathcal{D}$ we collected are independent observations from a specific probability distribution, $\mathcal{P}$, which is parameterised by $\theta$, i.e. $$\mathcal{D} =\{X_{1}, X_{2},\ldots,X_{n}\} \stackrel{iid}{\sim} \mathcal{P}_{\theta}$$

Further, we have some belief into the specific value of $\theta$, i.e. we think $\theta = c, c\in\mathbb{R}$. Unfortunately for us, we are dealing with randomness so it's very difficult to make statements along these lines. NHST was created to address this problem!

## NHST Intuition (specifically Fisherian)
Dealing with randomness makes life complicated, but there is no skirting this problem here. In fact, there may be a way to utilize our understanding of probability to our advantage and quantify our belief in how valid our belif is. For instance, if we had a coin that we somehow _knew_ to be fair and we flipped it 10 times, would we achieve 5 "Heads" exactly everytime? Certainly it would be the most probable single outcome ($P_{n = 10, \theta = 1/2}(X=5) \approx 24\%$), but it is far more likely that you _wouldn't_ get 5 "Heads" when you flip the coin. Yet, some outcomes of 10 flips of a perfectly fair coin seem fairly reasonable (e.g. getting 4 or 6 "Heads") while others would make you suspicious (e.g. getting 0 or 10 "Heads"). Being mathematicians, we always seek to quantify our uncertainty, and the underlying distribution of our sample can provide just that!

Keeping with the coin-flipping analogy, let's pretend that we flipped the coin 10 times and got the following results: $\mathcal{D} =\{H, H, T, T, H, H, T, H, H, T\}$. We believe that the coin is fair (i.e. $\theta = 1/2$), but is it reasonable to achieve 7 "Heads" in 10 flips with a fair coin? We could see how reasonable this result is by looking at the probability of getting something as extreme _or more_ than what we observed. Put mathematically, we want to find $P_{\theta = 1/2}(\mathrm{\#~of~Heads~\geq 7})$. If this probability is small, we would have reason to doubt our belief that the coin is fair. On the other hand, if the probability is large, we would feel more comfortable with our belief; in other words, the result would be fairly consistent with the coin being fair. To find the probability, we need to know the distribution over our sample. If we called flipping a "Head" a "Success", then (if the coin were fair) we know that the number of heads (which we'll define as the random variable "$X$") follows a Binomial distribution: $X\sim\mathcal{Bin}(10, 1/2)$. Then $$P(X \geq 7) \approx 17.2\%.$$
This means there was at least a 17% chance we would observe 7, 8, 9 or 10 "Heads" if the coin were fair. Now is this enough evidence for us to think the coin is unfair? Probably not, though anyone can pick whatever probability they wish! 

Now was simple as this example was, we _actually_ just completed a (Fisherian) analysis of our belief into the validity of dealing with a fair coin. Let's start attaching some terminology to what we just did!
* Our **Null Hypothesis** was that the coin was fair. Mathematically, $H_{0}: \theta = 1/2$. Because this was more of the approach taken by [Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) (rather than [Neyman](https://en.wikipedia.org/wiki/Jerzy_Neyman) & [Pearson](https://en.wikipedia.org/wiki/Egon_Pearson)), we didn't have an **Alternate hypothesis** that we were trying to decide between. In future posts, we'll talk more about this view, but for now, we just focussed on the believability of the Null Hypothesis by itself.
* When we collected our data, we didn't actually care about what flip resulted in what outcome; rather we _reduced_ all the information to a single statistic derived from the sample: the number of "Heads". This is a far more complex topic than I can cover in this post, but we only cared about the distribution of this specific **sample statistic**, rather than the distribution of the entire sample. This may seem overly pedantic, but I promise it's very important. 
* Given our sample statistic, we found the distribution of the _sample statistic_, known as the **Sample/Sampling Distribution**. This represents the distribution of all possible values we _could_ achieve in the context of our data collection. We'll see in a moment, but the sampling distribution can be very different from the data-generating distribution.
* Finally, when we looked at the probability of getting a sample statistic or something even more extreme, the resulting probability was our **p-value**. This is a very critical point! Very frequently (and as was the case at work), p-values are typically _misinterpreted_ as "the probability that Null Hypothesis is true". In reality, it is the probability of getting the results we had **or** something more extreme _if the Null Hypothesis is true_. It has nothing to do with the probability of the Null Hypothesis being true. Here the notion of "more extreme" will represent the smallest tail of the distribution and changes based on the sample. In this case, we looked at values greater than or equal to 7. Had we observed 1 "Head", then _more extreme_ would be looking at 0 or 1 "Heads". 

So in conclusion, a basic (Fisherian) NHST depends on fixing your Null Hypothesis (what you believe to be true), finding your sample statistic, and then looking at the probability of seeing what was observed or more extreme if the Null were true. That resulting probability is the p-value and can be used to make some decisions about the validity of the belief. 

To be clear, this is not the currently taught view of NHST which blends together this approach of Fisher with a decision-theoretic approach utilized by Neyman-Pearson. The current form that is typically taught directly considers an alternative hypothesis, considers the [**power**](https://en.wikipedia.org/wiki/Power_of_a_test) of the test and the two principal error types (Type I/Type II). These are important topics, but I'm not going to cover them in this post. However, I do want to cover a significant complication related to the use of the Sampling Distribution.

## Conceptual Issues
We showed that these tests require the use of a sampling distribution. In the above example, this was called out but it probably didn't seem like that big of a deal. But a major problem here (IMO) is that we made a very subtle assumption about how we tried to collect the sample and that this assumption plays a critical role. I said that we flipped a coin 10 times and got 7 "Heads". The implication is that I was going to flip the coin 10 times no matter what, and no matter how many "Heads" I got, I would stop at 10. But this was never clearly stated! We could just have easily decided to flip until we got 3 "Tails" and achieved this on the 10th flip. This very minor change in our goal _fundamentally_ changes the sampling distribution. Specifically, we would no longer have a Binomially distributed sample statistic! Rather, we would have a Negative Binomally distributed sample statistic! These distributions are not the same and can yield totally different p-values under the same set-up.

This is all to say that your intention when you collect your data has a profound impact on your results and this is almost never considered! This doesn't even begin to cover the case if we were going to flip two or more additional coins, where, _without even knowing the result of the other flips_, you would have totally different p-values just because you _intended_ to flip other coins. This is one of the craziest and most frustrating aspects of this theory in my opinion and in future posts I'll talk about other techniques that stay more closely related to the original data generating process and ignore this problem completely. Until then!